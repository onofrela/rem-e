"use client";

import { useState, useEffect, useCallback, useRef } from "react";
import { useRouter } from "next/navigation";
import { parseNavigationCommand, NavigationRoute } from "@/lib/voice/navigationCommands";
import {
  useKitchenContext,
  extractQuantity,
  extractLocation,
  isAssistantAskingQuestion
} from "./useKitchenContext";

export type VoiceStatus = "disconnected" | "listening" | "thinking" | "processing" | "error";

type ErrorType = "browser_not_supported" | "microphone_denied" | "unknown";

export interface LLMResponse {
  question: string;
  response: string;
  timestamp: number;
}

export interface VoiceError {
  type: ErrorType;
  message: string;
  suggestion: string;
}

interface UseVoiceNavigationReturn {
  status: VoiceStatus;
  transcript: string;
  lastCommand: string;
  lastNavigation: NavigationRoute | null;
  llmResponse: LLMResponse | null;
  error: VoiceError | null;
  executingFunction: string | null;
  connect: () => void;
  disconnect: () => void;
  clearResponse: () => void;
  clearError: () => void;
  updateContext: (context: VoiceContext) => void;
}

export interface VoiceContext {
  inventory?: string[];
  recipes?: string[];
  current_page?: string;
  // Recipe guide context
  inRecipeGuide?: boolean;
  recipeId?: string | null;
  recipeName?: string;
  currentStep?: number | null;
  currentStepInstruction?: string;
  currentStepIngredients?: string[];
  currentStepTip?: string;
  currentStepWarning?: string;
  currentStepDuration?: number;
  sessionId?: string | null;
}

// Helper para crear errores con sugerencias
function createVoiceError(type: ErrorType, message?: string): VoiceError {
  // Detectar si es m√≥vil para mensajes espec√≠ficos
  const isMobile = typeof window !== 'undefined' && /iPhone|iPad|iPod|Android/i.test(navigator.userAgent);

  const errors: Record<ErrorType, { message: string; suggestion: string }> = {
    browser_not_supported: {
      message: message || "Tu navegador no soporta reconocimiento de voz",
      suggestion: "Usa Chrome, Edge o Safari para usar el asistente de voz"
    },
    microphone_denied: {
      message: message || "Acceso al micr√≥fono denegado",
      suggestion: isMobile
        ? "En m√≥vil: Toca el bot√≥n del micr√≥fono para activar. Si no funciona, verifica permisos en Ajustes > Safari/Chrome > Micr√≥fono"
        : "Permite el acceso al micr√≥fono en la configuraci√≥n del navegador"
    },
    unknown: {
      message: message || "Error t√©cnico desconocido",
      suggestion: "Revisa la consola para m√°s detalles o recarga la p√°gina"
    }
  };

  return {
    type,
    message: errors[type].message,
    suggestion: errors[type].suggestion
  };
}

export function useVoiceNavigation(): UseVoiceNavigationReturn {
  const router = useRouter();
  const [status, setStatus] = useState<VoiceStatus>("disconnected");
  const [transcript, setTranscript] = useState("");
  const [lastCommand, setLastCommand] = useState("");
  const [lastNavigation, setLastNavigation] = useState<NavigationRoute | null>(null);
  const [llmResponse, setLlmResponse] = useState<LLMResponse | null>(null);
  const [error, setError] = useState<VoiceError | null>(null);
  const [executingFunction] = useState<string | null>(null);
  const [currentContext, setCurrentContext] = useState<VoiceContext>({});
  const [conversationId, setConversationId] = useState<string>('default');

  // IMPORTANTE: conversationMode usa ref para evitar stale closures
  // pero tambi√©n necesitamos un trigger state para el useEffect del timeout
  const conversationModeRef = useRef(false);
  const [conversationModeTrigger, setConversationModeTrigger] = useState(false);
  const lastLLMWasQuestionRef = useRef(false);

  const recognitionRef = useRef<any | null>(null);
  const isListeningRef = useRef(false);
  const wakeWordDetectedRef = useRef(false);
  const isProcessingCommandRef = useRef(false); // Nuevo: prevenir auto-restart durante procesamiento

  // ============================================================================
  // KITCHEN CONTEXT - Recuperado del servidor Python Vosk
  // ============================================================================
  const {
    context: kitchenContext,
    updateActivity,
    checkTimeout,
    setPendingIngredient,
    setPendingLocation,
    clearPending,
    hasAllDataForInventory,
    getInventoryData,
  } = useKitchenContext();

  // Actualizar contexto (ahora solo local, no enviamos a servidor)
  const updateContext = useCallback((context: VoiceContext) => {
    setCurrentContext(context);
    console.log("[Voice] Context updated:", context);
  }, []);

  // Detectar palabra de activaci√≥n "Rem-E"
  const detectWakeWord = useCallback((text: string): boolean => {
    const lowerText = text.toLowerCase().trim();
    return lowerText.includes('reme') ||
           lowerText.includes('rem e') ||
           lowerText.includes('remi') ||
           lowerText.startsWith('reme') ||
           lowerText.startsWith('rem e');
  }, []);

  // Extraer comando despu√©s de la palabra de activaci√≥n
  const extractCommand = useCallback((text: string): string => {
    const lowerText = text.toLowerCase();
    // Buscar "Rem-E" y tomar todo lo que viene despu√©s
    const patterns = ['reme', 'rem e', 'remi'];
    for (const pattern of patterns) {
      const index = lowerText.indexOf(pattern);
      if (index !== -1) {
        const command = text.substring(index + pattern.length).trim();
        return command;
      }
    }
    return text;
  }, []);
  // Clasificar intent usando el LLM
  const classifyWithLLM = useCallback(async (text: string): Promise<string> => {
    try {
      const res = await fetch('/api/assistant/classify', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ text })
      });

      const data = await res.json();
      console.log("[Voice] Clasificaci√≥n LLM:", data.classification);
      return data.classification; // NAVIGATION, INVENTORY_ACTION, APPLIANCE_ACTION, RECIPE_SEARCH, COOKING_CONTROL, GENERAL_QUESTION
    } catch (error) {
      console.error("[Voice] Error en clasificaci√≥n:", error);
      return 'GENERAL_QUESTION'; // Fallback
    }
  }, []);

  // Detectar comandos de cocina
  const detectCookingCommand = useCallback((text: string): string | null => {
    const lowerText = text.toLowerCase();

    // Comandos de navegaci√≥n de pasos
    if (lowerText.includes('siguiente') || lowerText.includes('continuar')) {
      return 'siguiente';
    }
    if (lowerText.includes('anterior') || lowerText.includes('atr√°s')) {
      return 'anterior';
    }
    if (lowerText.includes('repetir') || lowerText.includes('repite')) {
      return 'repetir';
    }
    if (lowerText.includes('pausar') || lowerText.includes('pausa')) {
      return 'pausar';
    }
    if (lowerText.includes('reanudar') || lowerText.includes('contin√∫a')) {
      return 'reanudar';
    }

    // Comando de timer
    if (lowerText.includes('timer') || lowerText.includes('temporizador') || lowerText.includes('cron√≥metro')) {
      return 'timer';
    }

    return null;
  }, []);

  // Procesar comando de navegaci√≥n
  const processNavigation = useCallback(
    (command: string) => {
      if (!command) return;

      setLastCommand(command);
      setStatus("processing");
      setLlmResponse(null);

      // Parsear comando de navegaci√≥n
      const route = parseNavigationCommand(command);
      if (route) {
        setLastNavigation(route);
        router.push(route.path);
      } else {
        setLastNavigation(null);
      }

      // Volver a estado de escucha
      setTimeout(() => {
        setStatus("listening");
        wakeWordDetectedRef.current = false;
      }, 500);
    },
    [router]
  );

  // Procesar comando de cocina
  const processCookingCommand = useCallback((cookingCmd: string, originalText: string) => {
    console.log("[Voice] Comando de cocina detectado:", cookingCmd);
    window.dispatchEvent(new CustomEvent('cooking-command', {
      detail: {
        command: cookingCmd,
        originalText
      }
    }));
    setStatus("listening");
    wakeWordDetectedRef.current = false;
  }, []);

  // Procesar comando basado en clasificaci√≥n LLM (con ejecuci√≥n de handlers en cliente)
  const processCommand = useCallback(async (text: string, isFollowUp: boolean = false) => {
    console.log("[Voice] Procesando comando:", text, "FollowUp:", isFollowUp);
    setLastCommand(text);
    setStatus("thinking");
    setLlmResponse(null);
    isProcessingCommandRef.current = true; // Marcar que estamos procesando

    // Variable para controlar si debemos activar modo conversaci√≥n al final
    let shouldActivateConversation = false;

    try {
      // =========================================================================
      // EXTRACCI√ìN DE DATOS - Recuperado del servidor Python
      // =========================================================================
      const detectedQuantity = extractQuantity(text);
      const detectedLocation = extractLocation(text);

      // Guardar cantidad si es diferente del default
      if (detectedQuantity > 1 && kitchenContext.pending_quantity !== detectedQuantity) {
        console.log(`[Kitchen] Cantidad detectada: ${detectedQuantity}`);
        // Actualizar directamente el pending_quantity
        kitchenContext.pending_quantity = detectedQuantity;
      }

      // Guardar ubicaci√≥n si se detect√≥
      if (detectedLocation) {
        console.log(`[Kitchen] Ubicaci√≥n detectada: ${detectedLocation}`);
        setPendingLocation(detectedLocation);
      }

      // Paso 1: Clasificar con el LLM (solo si no es follow-up)
      let classification = 'GENERAL_QUESTION';
      if (!isFollowUp) {
        classification = await classifyWithLLM(text);
        console.log("[Voice] Clasificaci√≥n obtenida:", classification);
      }

      // Paso 2: Manejar seg√∫n clasificaci√≥n
      if (classification === 'NAVIGATION' && !isFollowUp) {
        // Navegaci√≥n simple - usar el sistema de navegaci√≥n existente
        const route = parseNavigationCommand(text);
        if (route) {
          console.log("[Voice] Navegaci√≥n detectada:", route.path);
          setLastNavigation(route);
          router.push(route.path);
          setStatus("listening");
          wakeWordDetectedRef.current = false;
          return;
        }
      }

      // Paso 3: Para todo lo dem√°s, llamar al asistente con funciones (flujo h√≠brido)
      let currentText = text;
      let maxIterations = 5;
      let iterations = 0;

      while (iterations < maxIterations) {
        iterations++;

        const response = await fetch('/api/assistant', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({
            text: currentText,
            context: currentContext,
            conversationId: conversationId,
            classification: classification,
          }),
        });

        const data = await response.json();

        if (!data.success) {
          setError(createVoiceError("unknown", data.error || "Error al procesar la pregunta"));
          break;
        }

        // CASO A: El LLM devolvi√≥ una respuesta final (sin tool_calls)
        if (data.response) {
          setLlmResponse({
            question: text,
            response: data.response,
            timestamp: Date.now(),
          });

          // ===================================================================
          // DETECCI√ìN DE PREGUNTA - Recuperado del servidor Python
          // ===================================================================
          const isLLMQuestion = isAssistantAskingQuestion(data.response);

          if (isLLMQuestion) {
            conversationModeRef.current = true;
            setConversationModeTrigger(true);
            lastLLMWasQuestionRef.current = true;
            shouldActivateConversation = true; // Marcar para activar en finally

            // CR√çTICO: Resetear isProcessingCommandRef INMEDIATAMENTE
            // para permitir que onend reinicie el recognition si se detiene
            isProcessingCommandRef.current = false;

            console.log("[Voice] Modo conversaci√≥n ACTIVADO - LLM hizo una pregunta:", data.response);
          } else {
            if (!isFollowUp) {
              conversationModeRef.current = false;
              setConversationModeTrigger(false);
              lastLLMWasQuestionRef.current = false;
              shouldActivateConversation = false;
              clearPending(); // Limpiar pending si la conversaci√≥n termin√≥
              console.log("[Voice] Modo conversaci√≥n DESACTIVADO - respuesta completada");
            }
          }

          console.log("[Voice] Respuesta LLM:", data.response);
          break; // Salir del loop
        }

        // CASO B: El LLM solicita ejecutar tool_calls
        if (data.toolCallsPending && data.toolCallsPending.length > 0) {
          console.log("[Voice] Ejecutando herramientas en cliente:", data.toolCallsPending.map((tc: any) => tc.name).join(', '));

          // Importar din√°micamente el ejecutor de handlers del cliente
          const { executeClientFunction } = await import('@/lib/db/llm/clientHandlers');

          // Ejecutar cada tool_call en el cliente
          const toolResults = [];
          for (const toolCall of data.toolCallsPending) {
            console.log(`[Voice] Ejecutando: ${toolCall.name}`, toolCall.args);
            const result = await executeClientFunction(toolCall.name, toolCall.args);

            // =================================================================
            // MANEJO DE searchIngredients - Recuperado del servidor Python
            // =================================================================
            if (toolCall.name === 'searchIngredients' && result.success && result.data) {
              const ingredients = result.data;
              if (Array.isArray(ingredients) && ingredients.length > 0) {
                const firstIng = ingredients[0];
                console.log(`[Kitchen] ‚úÖ Ingrediente encontrado: ${firstIng.name} (ID: ${firstIng.id})`);

                // Guardar ingrediente pendiente con la cantidad ya extra√≠da
                setPendingIngredient(
                  { id: firstIng.id, name: firstIng.name },
                  kitchenContext.pending_quantity,
                  kitchenContext.pending_unit
                );

                // Verificar si ya tenemos todos los datos
                if (hasAllDataForInventory()) {
                  console.log("[Kitchen] ‚úÖ Todos los datos disponibles - Auto-agregando al inventario...");

                  const inventoryData = getInventoryData();
                  if (inventoryData) {
                    const addResult = await executeClientFunction('addToInventory', inventoryData);

                    if (addResult.success) {
                      console.log("[Kitchen] ‚úÖ Ingrediente agregado exitosamente");
                      clearPending();

                      // Agregar resultado de addToInventory tambi√©n
                      toolResults.push({
                        tool_call_id: `auto_add_${toolCall.id}`,
                        result: addResult,
                      });
                    } else {
                      console.error("[Kitchen] ‚ùå Error al agregar:", addResult.error);
                    }
                  }
                } else {
                  console.log("[Kitchen] ‚è≥ Falta ubicaci√≥n - el LLM preguntar√°");
                }
              }
            }

            // =================================================================
            // MANEJO DE searchRecipes - Auto-navegar a mejor coincidencia
            // =================================================================
            if (toolCall.name === 'searchRecipes' && result.success && result.data) {
              const searchData = result.data as {
                found?: boolean;
                count?: number;
                recipes?: Array<{ id: string; name: string; [key: string]: any }>;
                message?: string;
              };

              // El resultado de searchRecipes tiene formato: { found, count, recipes, message }
              if (searchData.found && searchData.recipes && searchData.recipes.length > 0) {
                console.log(`[Recipe] üîç Encontradas ${searchData.count} recetas`);

                // El LLM debe analizar los resultados y decidir cu√°l es la mejor coincidencia
                // Pero si solo hay 1 resultado, navegamos directamente
                if (searchData.recipes.length === 1) {
                  const recipe = searchData.recipes[0];
                  console.log(`[Recipe] ‚úÖ Solo 1 resultado - Auto-navegando a: ${recipe.name}`);

                  const navResult = await executeClientFunction('navigateToRecipe', {
                    recipeId: recipe.id,
                    recipeName: recipe.name
                  });

                  if (navResult.success && navResult.data) {
                    const navData = navResult.data as { url?: string };
                    if (navData.url) {
                      console.log("[Recipe] üöÄ Navegando a:", navData.url);
                      router.push(navData.url);
                    }
                  }

                  // Agregar resultado de navegaci√≥n
                  toolResults.push({
                    tool_call_id: `auto_nav_${toolCall.id}`,
                    result: navResult,
                  });
                }
                // Si hay m√∫ltiples resultados, el LLM decidir√° en la siguiente iteraci√≥n
                else {
                  console.log(`[Recipe] üìã M√∫ltiples resultados (${searchData.count}) - El LLM elegir√° la mejor coincidencia`);
                }
              }
            }

            // =================================================================
            // MANEJO DE navigateToRecipe - Navegaci√≥n autom√°tica
            // =================================================================
            if (toolCall.name === 'navigateToRecipe' && result.success && result.data) {
              const navData = result.data as { url?: string; recipeName?: string; recipeId?: string };
              if (navData.url) {
                console.log("[Recipe] üöÄ Navegando a receta:", navData.recipeName || navData.recipeId);
                console.log("[Recipe] URL:", navData.url);
                router.push(navData.url);
              }
            }

            toolResults.push({
              tool_call_id: toolCall.id,
              result,
            });
          }

          // Enviar resultados de vuelta al servidor
          const responseWithResults = await fetch('/api/assistant', {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
            },
            body: JSON.stringify({
              text: '', // No hay nuevo texto del usuario
              context: currentContext,
              conversationId: conversationId,
              toolResults, // Enviar resultados de las herramientas
            }),
          });

          const dataWithResults = await responseWithResults.json();

          if (!dataWithResults.success) {
            setError(createVoiceError("unknown", dataWithResults.error || "Error procesando resultados"));
            break;
          }

          // Si el servidor devuelve una respuesta final, mostrarla
          if (dataWithResults.response) {
            setLlmResponse({
              question: text,
              response: dataWithResults.response,
              timestamp: Date.now(),
            });

            const isLLMQuestion = isAssistantAskingQuestion(dataWithResults.response);

            if (isLLMQuestion) {
              conversationModeRef.current = true;
              setConversationModeTrigger(true);
              lastLLMWasQuestionRef.current = true;
              shouldActivateConversation = true; // Marcar para activar en finally

              // CR√çTICO: Resetear isProcessingCommandRef INMEDIATAMENTE
              // para permitir que onend reinicie el recognition si se detiene
              isProcessingCommandRef.current = false;

              console.log("[Voice] Modo conversaci√≥n ACTIVADO - LLM hizo una pregunta");
            } else {
              if (!isFollowUp) {
                conversationModeRef.current = false;
                setConversationModeTrigger(false);
                lastLLMWasQuestionRef.current = false;
                shouldActivateConversation = false;
                clearPending();
                console.log("[Voice] Modo conversaci√≥n DESACTIVADO");
              }
            }

            console.log("[Voice] Respuesta LLM final:", dataWithResults.response);
            break;
          }

          // Si el servidor solicita M√ÅS tool_calls, continuar el loop
          if (dataWithResults.toolCallsPending && dataWithResults.toolCallsPending.length > 0) {
            console.log("[Voice] LLM solicita m√°s herramientas, continuando...");
            currentText = ''; // Limpio porque es continuaci√≥n
            continue;
          }

          // No hay m√°s tool_calls ni respuesta, salir
          break;
        }

        // Si llegamos aqu√≠ sin respuesta ni tool_calls, salir
        break;
      }

    } catch (err) {
      console.error("[Voice] Error llamando al asistente:", err);
      setError(createVoiceError("unknown", "No se pudo conectar con el asistente"));
    } finally {
      // Guardar el valor antes de cualquier timeout
      const shouldRestart = shouldActivateConversation;

      // Esperar un poco antes de volver a listening para dar tiempo a que el LLM termine
      setTimeout(() => {
        setStatus("listening");
        wakeWordDetectedRef.current = false;

        // Solo resetear si NO fue reseteado antes (por detecci√≥n de pregunta)
        if (isProcessingCommandRef.current) {
          isProcessingCommandRef.current = false;
          console.log("[Voice] Resetting isProcessingCommandRef in finally");
        }

        // Si debemos activar modo conversaci√≥n, asegurarnos de que recognition est√© activo
        if (shouldRestart && recognitionRef.current) {
          // Si NO est√° escuchando, reiniciar
          if (!isListeningRef.current) {
            try {
              recognitionRef.current.start();
              isListeningRef.current = true;
              console.log("[Voice] ‚úÖ Restarted recognition for conversation mode");

              // ‚è±Ô∏è IMPORTANTE: Iniciar timeout AQU√ç, despu√©s de reiniciar recognition
              updateActivity();
              console.log("[Timeout] ‚è±Ô∏è Cuenta de 30s iniciada - el usuario puede responder ahora");
            } catch (err) {
              console.error("[Voice] Error restarting recognition:", err);
            }
          } else {
            // Ya est√° escuchando (el onend lo reinici√≥), solo activar timeout
            updateActivity();
            console.log("[Voice] ‚úÖ Recognition already active for conversation mode");
            console.log("[Timeout] ‚è±Ô∏è Cuenta de 30s iniciada - el usuario puede responder ahora");
          }
        }
      }, shouldRestart ? 1000 : 500); // M√°s tiempo si es conversaci√≥n
    }
  }, [
    currentContext,
    conversationId,
    classifyWithLLM,
    router,
    kitchenContext,
    setPendingIngredient,
    setPendingLocation,
    clearPending,
    hasAllDataForInventory,
    getInventoryData,
    updateActivity
  ]);

  // Manejar resultado de reconocimiento de voz
  const handleRecognitionResult = useCallback(
    (event: any) => {
      const results = event.results;
      const lastResult = results[results.length - 1];
      const transcriptText = lastResult[0].transcript;

      console.log("[Voice] Transcript:", transcriptText, "isFinal:", lastResult.isFinal);

      // Actualizar transcript en tiempo real
      setTranscript(transcriptText);

      // Solo procesar comandos finales
      if (lastResult.isFinal) {
        // Si estamos en una receta, primero intentar detectar comandos de cocina
        if (currentContext.inRecipeGuide) {
          const cookingCmd = detectCookingCommand(transcriptText);
          if (cookingCmd) {
            processCookingCommand(cookingCmd, transcriptText);
            setTranscript("");
            return;
          }
        }

        // Modo conversaci√≥n: permite responder sin wake word
        // IMPORTANTE: Esto debe verificarse ANTES de buscar wake word
        if (conversationModeRef.current && lastLLMWasQuestionRef.current) {
          console.log("[Voice] Conversation mode active, processing without wake word:", transcriptText);

          // En modo conversaci√≥n, asumimos que el usuario est√° respondiendo a la pregunta del LLM
          // Por lo tanto, NO clasificamos como navegaci√≥n, sino como pregunta de seguimiento
          processCommand(transcriptText, true); // true = es follow-up

          setTranscript("");
          // NO resetear wakeWordDetectedRef aqu√≠, dejarlo como est√°
          return;
        }

        // Si a√∫n no detectamos palabra de activaci√≥n, buscarla
        if (!wakeWordDetectedRef.current) {
          if (detectWakeWord(transcriptText)) {
            wakeWordDetectedRef.current = true;
            const command = extractCommand(transcriptText);

            if (command) {
              // Si hay comando inmediatamente despu√©s de wake word, procesarlo
              console.log("[Voice] Wake word detected with command:", command);
              processCommand(command); // Clasificaci√≥n se hace dentro de processCommand
            } else {
              // Solo wake word, esperar comando
              console.log("[Voice] Wake word detected, waiting for command");
              setLastCommand("(esperando comando...)");
            }
            setTranscript("");
          }
        } else {
          // Ya detectamos wake word, este es el comando
          console.log("[Voice] Processing command after wake word:", transcriptText);
          // Si est√°bamos en conversaci√≥n pero el usuario us√≥ wake word, se resetea
          conversationModeRef.current = false;
          setConversationModeTrigger(false);
          lastLLMWasQuestionRef.current = false;

          // Procesar comando (clasificaci√≥n se hace dentro)
          processCommand(transcriptText);
          setTranscript("");
        }
      }
    },
    [detectWakeWord, extractCommand, processCommand, detectCookingCommand, processCookingCommand, currentContext.inRecipeGuide]
  );

  // Limpiar respuesta del LLM
  const clearResponse = useCallback(() => {
    setLlmResponse(null);
    setLastCommand("");
    conversationModeRef.current = false;
    setConversationModeTrigger(false);
    lastLLMWasQuestionRef.current = false;
    // Generar nuevo ID de conversaci√≥n para empezar un nuevo hilo
    setConversationId(`conv_${Date.now()}`);
    console.log("[Voice] Conversaci√≥n limpiada - nuevo ID generado");
  }, []);

  // Limpiar error
  const clearError = useCallback(() => {
    setError(null);
  }, []);

  // Conectar (iniciar reconocimiento de voz)
  const connect = useCallback(async () => {
    if (typeof window === 'undefined') return;

    // Verificar si estamos en contexto seguro (HTTPS o localhost)
    const isSecureContext = window.isSecureContext;
    if (!isSecureContext) {
      console.error("[Voice] Not in secure context (HTTPS required)");
      setError(createVoiceError("unknown", "Se requiere HTTPS para usar el micr√≥fono. Accede desde https:// o localhost"));
      setStatus("error");
      return;
    }

    // Verificar soporte del navegador
    const SpeechRecognitionAPI = window.SpeechRecognition || window.webkitSpeechRecognition;

    if (!SpeechRecognitionAPI) {
      setError(createVoiceError("browser_not_supported"));
      setStatus("error");
      return;
    }

    if (isListeningRef.current) {
      console.log("[Voice] Already listening");
      return;
    }

    // IMPORTANTE: En tablets/m√≥viles, solicitar permisos EXPL√çCITAMENTE primero
    // Esto fuerza al navegador a mostrar el di√°logo de permisos
    try {
      console.log("[Voice] Requesting microphone permission...");
      console.log("[Voice] Current URL:", window.location.href);
      console.log("[Voice] Is secure context:", isSecureContext);

      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

      // Inmediatamente detener el stream, solo lo usamos para obtener permisos
      stream.getTracks().forEach(track => track.stop());
      console.log("[Voice] Microphone permission granted");
    } catch (permError: any) {
      console.error("[Voice] Microphone permission denied:", permError);
      setError(createVoiceError("microphone_denied", permError.message));
      setStatus("error");
      return;
    }

    try {
      const recognition = new SpeechRecognitionAPI();
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = 'es-MX';

      recognition.onresult = handleRecognitionResult;

      recognition.onerror = (event: any) => {
        console.error('[Voice] Speech recognition error:', event.error);

        if (event.error === 'not-allowed' || event.error === 'permission-denied') {
          setError(createVoiceError("microphone_denied"));
          setStatus("error");
          isListeningRef.current = false;
        } else if (event.error === 'no-speech') {
          // No es un error real, solo continuar
          console.log("[Voice] No speech detected, continuing...");
          // No cambiar el estado, permitir que siga escuchando
        } else if (event.error === 'aborted') {
          // Usuario cancel√≥ o navegador bloque√≥
          console.log("[Voice] Recognition aborted");
          setStatus("disconnected");
          isListeningRef.current = false;
        } else {
          setError(createVoiceError("unknown", `Error: ${event.error}`));
          setStatus("error");
          isListeningRef.current = false;
        }
      };

      recognition.onend = () => {
        console.log("[Voice] Recognition ended, isProcessing:", isProcessingCommandRef.current);
        isListeningRef.current = false;

        // NO auto-restart si estamos procesando un comando (el processCommand lo har√°)
        if (isProcessingCommandRef.current) {
          console.log("[Voice] Skipping auto-restart (processing command)");
          return;
        }

        // Auto-restart si el recognition existe y no estamos desconectados/error
        // Usamos recognitionRef para verificar que no fue desconectado manualmente
        if (recognitionRef.current) {
          setTimeout(() => {
            // Verificar nuevamente que no estamos procesando (puede haber cambiado)
            if (recognitionRef.current && !isProcessingCommandRef.current) {
              try {
                recognitionRef.current.start();
                isListeningRef.current = true;
                console.log("[Voice] Auto-restarted recognition");
              } catch (err) {
                console.error("[Voice] Error auto-restarting:", err);
              }
            } else {
              console.log("[Voice] Skipped auto-restart - processing or disconnected");
            }
          }, 100);
        }
      };

      recognitionRef.current = recognition;
      recognition.start();
      isListeningRef.current = true;
      wakeWordDetectedRef.current = false;
      setStatus("listening");
      setError(null);
      console.log("[Voice] Started listening");
    } catch (err) {
      console.error('[Voice] Error starting recognition:', err);
      setError(createVoiceError("unknown", String(err)));
      setStatus("error");
      isListeningRef.current = false;
    }
  }, [handleRecognitionResult, status]);

  // Desconectar (detener reconocimiento de voz)
  const disconnect = useCallback(() => {
    if (recognitionRef.current) {
      try {
        recognitionRef.current.stop();
        recognitionRef.current = null;
      } catch (err) {
        console.error("[Voice] Error stopping recognition:", err);
      }
    }
    isListeningRef.current = false;
    wakeWordDetectedRef.current = false;
    setStatus("disconnected");
    setTranscript("");
    console.log("[Voice] Disconnected");
  }, []);

  // ============================================================================
  // TIMEOUT AUTOM√ÅTICO DE CONVERSACI√ìN - Recuperado del servidor Python
  // ============================================================================
  useEffect(() => {
    if (!conversationModeTrigger) return;

    console.log("[Timeout] Iniciando verificaci√≥n de timeout de conversaci√≥n");

    const interval = setInterval(() => {
      if (checkTimeout()) {
        console.log("[Timeout] Conversaci√≥n expirada - desactivando modo conversaci√≥n");
        conversationModeRef.current = false;
        setConversationModeTrigger(false);
        lastLLMWasQuestionRef.current = false;
        clearPending();
      }
    }, 5000); // Verificar cada 5 segundos

    return () => {
      clearInterval(interval);
    };
  }, [conversationModeTrigger, checkTimeout, clearPending]);

  // Auto-conectar al montar SOLO en desktop
  // En m√≥vil REQUIERE interacci√≥n del usuario
  useEffect(() => {
    // Detectar si es m√≥vil
    const isMobile = /iPhone|iPad|iPod|Android/i.test(navigator.userAgent);

    if (!isMobile) {
      // Solo auto-conectar en desktop
      const timer = setTimeout(() => {
        connect();
      }, 500);

      return () => {
        clearTimeout(timer);
        disconnect();
      };
    }

    // En m√≥vil, solo cleanup al desmontar
    return () => {
      disconnect();
    };
  }, []); // Solo al montar/desmontar

  return {
    status,
    transcript,
    lastCommand,
    lastNavigation,
    llmResponse,
    error,
    executingFunction,
    connect,
    disconnect,
    clearResponse,
    clearError,
    updateContext,
  };
}
